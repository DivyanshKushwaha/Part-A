{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jatin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing stopwords, punctuations, and tokenizing it.\n",
    "    \"\"\"\n",
    "    # Remove special characters & numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize words and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahul wakes early every day goes college morning comes back pm present rahul outside buy snacks us\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"Rahul wakes up early every day. He goes to college in the morning and comes back at 3 pm. At present, Rahul is outside. He has to buy the snacks for all of us.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "cleaned_text = preprocess_text(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rahul wakes up \\nearly every day.', 'He goes to college in the morning and comes back at 3 pm.', 'At present, Rahul is \\noutside.', 'He has to buy the snacks for all of us.']\n"
     ]
    }
   ],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into sentences using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "# Tokenizing sentences\n",
    "sentences = get_sentences(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He has to buy the snacks for all of us.', 'Rahul')]\n"
     ]
    }
   ],
   "source": [
    "task_keywords = {\"must\", \"should\", \"has to\", \"needs to\", \"required to\", \"is to\", \"have to\", \"is supposed to\"}\n",
    "\n",
    "def extract_tasks(sentences):\n",
    "    \"\"\"\n",
    "    Extracts sentences that contain task indicators and tracks the last mentioned entity.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    last_subject = None  # Store last identified person\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()  # Convert to lowercase for better matching\n",
    "\n",
    "        # Check for proper nouns (names like \"Rahul\") before checking tasks\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PROPN\":  \n",
    "                last_subject = token.text  # Update the last known subject\n",
    "        \n",
    "        # If the sentence contains a task keyword, store it\n",
    "        if any(keyword in sentence for keyword in task_keywords):\n",
    "            tasks.append((sentence, last_subject))  # Store the sentence along with the subject\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "\n",
    "tasks = extract_tasks(sentences)\n",
    "print(tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity and deadline extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'He has to buy the snacks for all of us.': 'Rahul'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_task_entity(task_tuple, last_subject=None):\n",
    "    sentence, last_subject = task_tuple\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract named entities labeled as PERSON\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    if entities:\n",
    "        return entities[0]\n",
    "\n",
    "    # Fallback to proper nouns in SpaCy\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "    if proper_nouns:\n",
    "        return proper_nouns[0]\n",
    "\n",
    "    # Use NLTK NER as a backup\n",
    "    nltk_entities = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    for subtree in nltk_entities:\n",
    "        if isinstance(subtree, nltk.Tree) and subtree.label() == \"PERSON\":\n",
    "            return \" \".join([token for token, pos in subtree.leaves()])\n",
    "\n",
    "    return last_subject\n",
    "\n",
    "task_entities = {task[0]: get_task_entity(task) for task in tasks}\n",
    "print(task_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'He has to buy the snacks for all of us.': 'No deadline specified'}\n"
     ]
    }
   ],
   "source": [
    "def get_task_deadline(sentence):\n",
    "    \"\"\"\n",
    "    Extracts all deadline-related information.\n",
    "    \"\"\"\n",
    "    match=None\n",
    "    doc = nlp(sentence)\n",
    "    deadlines = [ent.text for ent in doc.ents if ent.label_ in [\"DATE\", \"TIME\"]]\n",
    "    if \"before\" in sentence or \"by\" in sentence or \"until\" in sentence:\n",
    "        match = re.search(r\"(before|by|until)\\s+([0-9A-Za-z\\s]+)\", sentence)\n",
    "    if match:\n",
    "        deadlines.append(match.group(0))  # Capture \"before 5 PM\"\n",
    "    return \", \".join(deadlines) if deadlines else \"No deadline specified\"\n",
    "\n",
    "\n",
    "\n",
    "task_deadlines = {task[0]: get_task_deadline(task[0]) for task in tasks}\n",
    "print(task_deadlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'He has to buy the snacks for all of us.': 'Shopping'}\n"
     ]
    }
   ],
   "source": [
    "task_categories = {\n",
    "    \"Work\":[\"report\", \"assignment\", \"presentation\", \"submit\", \"form\"],\n",
    "    \"Shopping\": [\"buy\", \"purchase\"],\n",
    "    \"Cleaning\": [\"clean\", \"wash\"],\n",
    "    \"Communication\": [\"call\", \"email\", \"message\"]\n",
    "}\n",
    "\n",
    "def categorize_task(sentence):\n",
    "    \"\"\"\n",
    "    Categorizes tasks based on predefined keyword mappings.\n",
    "    \"\"\"\n",
    "    sentence_lower = str(sentence).lower()  # Ensure it's a string\n",
    "    for category, keywords in task_categories.items():\n",
    "        if any(word in sentence_lower for word in keywords):\n",
    "            return category\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "# Ensure tasks is a list of strings, not tuples\n",
    "task_categories = {task[0]: categorize_task(task[0]) for task in tasks}  # Use task[0] if tasks contain tuples\n",
    "print(task_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Task': 'has to buy the snacks for all of us.', 'Who': 'Rahul', 'Deadline': 'No deadline specified', 'Category': 'Shopping'}\n"
     ]
    }
   ],
   "source": [
    "def generate_task_summary(tasks, task_entities, task_deadlines, task_categories):\n",
    "    \"\"\"\n",
    "    Generates structured output for extracted tasks.\n",
    "    \"\"\"\n",
    "    extracted_tasks = []\n",
    "    for task in tasks:\n",
    "        task_sentence = task[0]  # Extract the actual sentence\n",
    "        if task[1]:  # If a valid subject is found (e.g., Rahul)\n",
    "            task_text = re.sub(rf\"\\b({re.escape(task[1])}|he|she|they|it)\\b\", \"\", task_sentence, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        extracted_tasks.append({\n",
    "\n",
    "            \"Task\": task_text,\n",
    "            \"Who\": task_entities.get(task_sentence, \"Unknown\"),\n",
    "            \"Deadline\": task_deadlines.get(task_sentence, \"No deadline\"),\n",
    "            \"Category\": task_categories.get(task_sentence, \"Uncategorized\")\n",
    "        })\n",
    "    \n",
    "    return extracted_tasks\n",
    "\n",
    "\n",
    "structured_tasks = generate_task_summary(tasks, task_entities, task_deadlines, task_categories)\n",
    "for task in structured_tasks:\n",
    "    print(task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaidemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
